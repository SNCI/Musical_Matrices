{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danshiebler/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "sys.path.append(\"music_rnn\")\n",
    "sys.path.append(\"/Users/danshiebler/Documents\")\n",
    "import helper_functions as hf\n",
    "import pygame\n",
    "from matplotlib import pyplot as plt\n",
    "import midi_util\n",
    "import feature_encoder\n",
    "import music_lstm\n",
    "import time\n",
    "#RATHER THAN USE A HUGE DIRECT INTEGER ENCODING - TRY A DICTIONARY MAPPING\n",
    "\n",
    "def dump_midi(midi_vec, filename, time_step=32, resolution=480, n=128):\n",
    "    M  = midi_util.MidiWriter()\n",
    "    M.dump_sequence_to_midi(midi_vec, filename, time_step, resolution)\n",
    "\n",
    "def plot_midi(midi_vec, n=128):\n",
    "    plt.imshow(midi_vec, extent=[0, 1, 0, 1])\n",
    "\n",
    "train_files = !ls /Users/danshiebler/Documents/music_all/train\n",
    "test_files = !ls /Users/danshiebler/Documents/music_all/test\n",
    "cv_files = !ls /Users/danshiebler/Documents/music_all/valid\n",
    "\n",
    "resolution = 480\n",
    "time_step = 120\n",
    "music_all_path = \"/Users/danshiebler/Documents/music_all\"\n",
    "train_examples = [midi_util.parse_midi_to_sequence(\"{}/train/{}\".format(music_all_path, f), \n",
    "                                                   time_step=time_step) for f in train_files]\n",
    "test_examples = [midi_util.parse_midi_to_sequence(\"{}/test/{}\".format(music_all_path, f), \n",
    "                                                  time_step=time_step) for f in test_files[:5]]\n",
    "# cv_examples = [midi_util.parse_midi_to_sequence(\"{}/valid/{}\".format(music_all_path, f), time_step=time_step) for f in cv_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_hidden' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4b052d7f4a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Define weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m weights = {\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;34m'out'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m }\n\u001b[1;32m     18\u001b[0m biases = {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_hidden' is not defined"
     ]
    }
   ],
   "source": [
    "#run once\n",
    "timestep = 200\n",
    "learning_rate = 0.001\n",
    "n_steps = timestep\n",
    "n_classes = train_examples[0].shape[1]\n",
    "hidden_sizes = [128, 128] # hidden layer num of features\n",
    "\n",
    "        \n",
    "        \n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_classes])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = music_lstm.RNN(x, weights, biases, n_classes, timestep, hidden_sizes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "print \"complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "# sess.close()\n",
    "\n",
    "display_step = 10\n",
    "NUM_THREADS  = 4\n",
    "start = time.time()\n",
    "sess  = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=NUM_THREADS))\n",
    "sess.run(init)\n",
    "\n",
    "# Keep training until reach max iterations\n",
    "for step, ex in enumerate(train_examples):\n",
    "    batch_x, batch_y = music_lstm.get_forecast_Xy(ex, timestep)\n",
    "    if not len(batch_x):\n",
    "        continue\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "    if step % display_step == 0:\n",
    "        # Calculate batch accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "        # Calculate batch loss\n",
    "        loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "        print \"Iter \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.5f}\".format(acc) + \", Time Elapsed: {}\".format(time.time() - start)\n",
    "print \"Optimization Finished! {} seconds elapsed\".format(time.time() - start)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primer = music_lstm.get_forecast_Xy(test_examples[0], timestep)[0][2]\n",
    "predictions = music_lstm.compose(x, sess, pred, primer, steps=100)\n",
    "plot_midi(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump_midi(predictions, \"music_outputs/composition.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dump_midi(train_examples[0], \"music_outputs/sample.midi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
